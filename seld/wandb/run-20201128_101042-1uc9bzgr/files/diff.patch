diff --git a/configs/ein_seld/seld.yaml b/configs/ein_seld/seld.yaml
index 17668e3..18c291f 100644
--- a/configs/ein_seld/seld.yaml
+++ b/configs/ein_seld/seld.yaml
@@ -1,8 +1,8 @@
 method: ein_seld
 dataset: dcase2020task3
-workspace_dir: ./
-dataset_dir: ./_dataset/dataset_root
-hdf5_dir: ./_hdf5
+workspace_dir: /home/alrawis/EIN-SELD/
+dataset_dir: /home/alrawis/EIN-SELD/_dataset/dataset_root/
+hdf5_dir: /home/alrawis/EIN-SELD/_hdf5/
 data:
   type: foa
   sample_rate: 24000
@@ -19,16 +19,16 @@ data:
   audio_feature: logmel&intensity
   feature_freeze: True
 data_augmentation: 
-  type: None
+  type: train_rotate_channel #train_rotate_channel #pitchshift
 training:
   train_id: EINV2_tPIT_n1
   model: EINV2
-  resume_model: # None_epoch_latest.pth
+  resume_model:  # None_epoch_latest.pth
   loss_type: all
   loss_beta: 0.5
   PIT_type: tPIT
   batch_size: 32
-  train_fold: 2,3,4,5,6
+  train_fold: 2,3,4,5,6 # this corresponds to the eval mode in seld dcase2020
   valid_fold: 1
   overlap: 1&2
   optimizer: adam
diff --git a/scripts/evaluate.sh b/scripts/evaluate.sh
index bbdc45e..bc0273c 100644
--- a/scripts/evaluate.sh
+++ b/scripts/evaluate.sh
@@ -4,4 +4,4 @@ set -e
 
 CONFIG_FILE='./configs/ein_seld/seld.yaml'
 
-python seld/main.py -c $CONFIG_FILE evaluate
\ No newline at end of file
+python3 seld/main.py -c $CONFIG_FILE evaluate
\ No newline at end of file
diff --git a/scripts/predict.sh b/scripts/predict.sh
index 7524d99..04a659b 100644
--- a/scripts/predict.sh
+++ b/scripts/predict.sh
@@ -4,4 +4,4 @@ set -e
 
 CONFIG_FILE='./configs/ein_seld/seld.yaml'
 
-python seld/main.py -c $CONFIG_FILE infer --num_workers=8
\ No newline at end of file
+python3 seld/main.py -c $CONFIG_FILE infer --num_workers=4
\ No newline at end of file
diff --git a/scripts/train.sh b/scripts/train.sh
index 21d302a..90845b7 100644
--- a/scripts/train.sh
+++ b/scripts/train.sh
@@ -2,6 +2,6 @@
 
 set -e
 
-CONFIG_FILE='./configs/ein_seld/seld.yaml'
+CONFIG_FILE='/home/alrawis/EIN-SELD/configs/ein_seld/seld.yaml'
 
-python seld/main.py -c $CONFIG_FILE train --seed=$(shuf -i 0-10000 -n 1) --num_workers=8
+python3 seld/main.py -c $CONFIG_FILE train --seed=$(shuf -i 0-10000 -n 1) --num_workers=8
\ No newline at end of file
diff --git a/seld/learning/evaluate.py b/seld/learning/evaluate.py
index 10dc1fb..bf682ff 100644
--- a/seld/learning/evaluate.py
+++ b/seld/learning/evaluate.py
@@ -11,6 +11,7 @@ from methods.utils.SELD_evaluation_metrics_2020 import \
 from methods.utils.SELD_evaluation_metrics_2020 import early_stopping_metric
 
 
+
 def evaluate(cfg, dataset):
 
     """ Evaluate scores
diff --git a/seld/learning/initialize.py b/seld/learning/initialize.py
index f9fbc41..be555a5 100644
--- a/seld/learning/initialize.py
+++ b/seld/learning/initialize.py
@@ -18,6 +18,7 @@ from utils.config import (get_afextractor, get_generator, get_losses,
 from learning.checkpoint import CheckpointIO
 
 
+
 def init_train(args, cfg, dataset):
     """ Training initialization.
 
@@ -36,9 +37,10 @@ def init_train(args, cfg, dataset):
     cudnn.deterministic = True
     cudnn.benchmark = True    
 
+
     '''Directories'''
     print('Train ID is {}\n'.format(cfg['training']['train_id']))
-    out_train_dir = Path(cfg['workspace_dir']).joinpath('out_train') \
+    out_train_dir = Path(cfg['workspace_dir']).joinpath('out_train_d_error'+ str(cfg['data']['type'])) \
         .joinpath(cfg['method']).joinpath(cfg['training']['train_id'])
     if out_train_dir.is_dir():
         flag = input("Train ID folder {} is existed, delete it? (y/n)". \
@@ -146,7 +148,7 @@ def init_infer(args, cfg, dataset):
     """
 
     '''Cuda'''
-    args.cuda = not args.no_cuda and torch.cuda.is_available() 
+    args.cuda = not args.no_cuda and torch.cuda.is_available()
 
     '''Directories'''
     print('Inference ID is {}\n'.format(cfg['inference']['infer_id']))
@@ -161,6 +163,7 @@ def init_infer(args, cfg, dataset):
     ckpts_paths_list = []
     ckpts_models_list = []
     for train_id, model_name in zip(train_ids, models):
+        #/home/alrawis/EIN-SELD/out_train/ein_seld/EINV2_tPIT_n1/checkpoints
         ckpts_dir = Path(cfg['workspace_dir']).joinpath('out_train').joinpath(cfg['method']) \
             .joinpath(train_id).joinpath('checkpoints')
         ckpt_path = [path for path in sorted(ckpts_dir.iterdir()) if path.stem.split('_')[-1].isnumeric()]
diff --git a/seld/learning/preprocess.py b/seld/learning/preprocess.py
index 8009e46..c7e8750 100644
--- a/seld/learning/preprocess.py
+++ b/seld/learning/preprocess.py
@@ -15,6 +15,7 @@ from utils.common import float_samples_to_int16
 from utils.config import get_afextractor
 
 
+
 class Preprocessor:
     """Preprocess the audio data.
 
diff --git a/seld/learning/train.py b/seld/learning/train.py
index d0485ab..b40887c 100644
--- a/seld/learning/train.py
+++ b/seld/learning/train.py
@@ -4,12 +4,16 @@ from timeit import default_timer as timer
 from tqdm import tqdm
 
 from utils.common import print_metrics
+import wandb
 
 
 def train(cfg, **initializer):
     """Train
 
     """
+    # log with wandb
+    dict_cofig = cfg
+    wandb.init(project="ein-2021", config=dict_cofig , entity='newseld')
     writer = initializer['writer']
     train_generator = initializer['train_generator']
     valid_generator = initializer['valid_generator']
@@ -39,7 +43,7 @@ def train(cfg, **initializer):
             train_losses = trainer.validate_step(valid_type='train', epoch_it=epoch_it)
             for k, v in train_losses.items():
                 train_losses[k] = v / batchNum_per_epoch
-
+                wandb.log({k: v / batchNum_per_epoch})
             if cfg['training']['valid_fold']:
                 valid_losses, valid_metrics = trainer.validate_step(
                     generator=valid_generator,
@@ -48,6 +52,19 @@ def train(cfg, **initializer):
                 )
             valid_time = timer() - valid_begin_time
 
+            for k, v in valid_losses.items():
+                wandb.log({k: v })
+            wandb.log({'Er20': valid_metrics['ER20'] })
+            wandb.log({'F20': valid_metrics['F20'] })
+            wandb.log({'LE20': valid_metrics['LE20'] })
+            wandb.log({'LR20': valid_metrics['LR20'] })
+            wandb.log({'seld20': valid_metrics['seld20'] })
+            wandb.log({'Er19': valid_metrics['ER19'] })
+            wandb.log({'F19': valid_metrics['F19'] })
+            wandb.log({'LE19': valid_metrics['LE19'] })
+            wandb.log({'LR19': valid_metrics['LR19'] })
+            wandb.log({'seld19': valid_metrics['seld19'] })
+            
             writer.add_scalar('train/lr', lr_scheduler.get_last_lr()[0], it)
             logging.info('---------------------------------------------------------------------------------------------------'
                 +'------------------------------------------------------')
diff --git a/seld/main.py b/seld/main.py
index b22ac75..8714b37 100644
--- a/seld/main.py
+++ b/seld/main.py
@@ -8,11 +8,10 @@ from utils.config import get_dataset
 
 def main(args, cfg):
     """Execute a task based on the given command-line arguments.
-    
+
     This function is the main entry-point of the program. It allows the
     user to extract features, train a model, infer predictions, and
     evaluate predictions using the command-line interface.
-
     Args:
         args: command line arguments.
     Return:
@@ -46,7 +45,7 @@ def main(args, cfg):
     elif args.mode == 'infer':
         infer_initializer = initialize.init_infer(args, cfg, dataset)
         infer.infer(cfg, dataset, **infer_initializer)
-    
+
     # Evaluate
     elif args.mode == 'evaluate':
         evaluate.evaluate(cfg, dataset)
@@ -56,4 +55,4 @@ def main(args, cfg):
 
 if __name__ == '__main__':
     args, cfg = parse_cli_overides()
-    sys.exit(main(args, cfg))
+    sys.exit(main(args, cfg))
\ No newline at end of file
diff --git a/seld/methods/ein_seld/data.py b/seld/methods/ein_seld/data.py
index de29762..4185cfe 100644
--- a/seld/methods/ein_seld/data.py
+++ b/seld/methods/ein_seld/data.py
@@ -6,6 +6,9 @@ import numpy as np
 import torch
 from methods.utils.data_utilities import (_segment_index, load_dcase_format,
                                           to_metrics2020_format)
+from methods.ein_seld.data_augmentation.pitch_shift import apply_pitch_shift
+from methods.ein_seld.data_augmentation.channel_rotation import *
+
 from torch.utils.data import Dataset, Sampler
 from tqdm import tqdm
 from utils.common import int16_samples_to_float32
@@ -25,7 +28,6 @@ class UserDataset(Dataset):
             overlap: '1' | '2'
         """
         super().__init__()
-
         self.dataset_type = dataset_type
         self.read_into_mem = args.read_into_mem
         self.sample_rate = cfg['data']['sample_rate']
@@ -64,6 +66,16 @@ class UserDataset(Dataset):
             self.paths_list = [path for data_dir in data_dirs for path in sorted(data_dir.glob('*.h5')) \
                 if int(path.stem[fold_str_idx]) in train_fold and path.stem[ov_str_idx] in ov_set \
                     and not path.name.startswith('.')]
+        elif self.dataset_type == 'train_rotate_channel':
+            # apply dataaugment
+            data_dirs = [dev_data_dir]
+            self.meta_dir = dev_meta_dir
+            train_fold = [int(fold.strip()) for fold in str(cfg['training']['train_fold']).split(',')]
+            ov_set = str(cfg['training']['overlap']) if not overlap else overlap
+            self.paths_list = [path for data_dir in data_dirs for path in sorted(data_dir.glob('*.h5')) \
+                               if int(path.stem[fold_str_idx]) in train_fold and path.stem[ov_str_idx] in ov_set \
+                               and not path.name.startswith('.')]
+            #apply_pitch_shif(self.paths_list,cfg)
         elif self.dataset_type == 'valid':
             if cfg['training']['valid_fold'] != 'eval':
                 data_dirs = [dev_data_dir]
@@ -113,6 +125,9 @@ class UserDataset(Dataset):
                 if not path.name.startswith('.')]
         self.paths_list = [Path(str(path) + '%' + str(n)) for path in self.paths_list for n in range(self.num_segments)]
 
+
+
+
         # Read into memory
         if self.read_into_mem:
             load_begin_time = timer()
@@ -184,13 +199,21 @@ class UserDataset(Dataset):
         else:
             path = self.paths_list[idx]
             fn, n_segment = path.stem, int(path.name.split('%')[1])
-            data_path = Path(str(path).split('%')[0])   
+            data_path = Path(str(path).split('%')[0])
             index_begin = self.segmented_indexes[n_segment][0]
             index_end = self.segmented_indexes[n_segment][1]
             pad_width_before = self.segmented_pad_width[n_segment][0]
             pad_width_after = self.segmented_pad_width[n_segment][1]
-            with h5py.File(data_path, 'r') as hf:
-                x = int16_samples_to_float32(hf['waveform'][:, index_begin: index_end])
+            if self.dataset_type == 'train_pitchshift':
+                with h5py.File(data_path, 'r') as hf:
+                    x = apply_pitch_shift(hf['waveform'][:, index_begin: index_end], self.sample_rate)
+            elif self.dataset_type == 'train_rotate_channel':
+                with h5py.File(data_path, 'r') as hf:
+                    x = int16_samples_to_float32(hf['waveform'][:, index_begin: index_end])
+                    x, pattern = apply_data_channel_rotation('foa',x)
+            else:
+                with h5py.File(data_path, 'r') as hf:
+                    x = int16_samples_to_float32(hf['waveform'][:, index_begin: index_end])
             pad_width = ((0, 0), (pad_width_before, pad_width_after))                    
             x = np.pad(x, pad_width, mode='constant')
             if 'test' not in self.dataset_type:
@@ -200,9 +223,14 @@ class UserDataset(Dataset):
                 # pad_width_before_label = int(pad_width_before / (self.sample_rate * self.label_resolution))
                 pad_width_after_label = int(pad_width_after / (self.sample_rate * self.label_resolution))
                 meta_path = self.meta_dir.joinpath(fn + '.h5')
-                with h5py.File(meta_path, 'r') as hf:
-                    sed_label = hf['sed_label'][index_begin_label: index_end_label, ...]
-                    doa_label = hf['doa_label'][index_begin_label: index_end_label, ...] # NOTE: this is Catesian coordinates
+                if self.dataset_type == 'train_rotate_channel':
+                    with h5py.File(meta_path, 'r') as hf:
+                        sed_label = hf['sed_label'][index_begin_label: index_end_label, ...]
+                        doa_label = apply_label_channel_rotation('foa',hf['doa_label'][index_begin_label: index_end_label, ...],pattern)
+                else:
+                    with h5py.File(meta_path, 'r') as hf:
+                        sed_label = hf['sed_label'][index_begin_label: index_end_label, ...]
+                        doa_label = hf['doa_label'][index_begin_label: index_end_label, ...] # NOTE: this is Catesian coordinates
                 if pad_width_after_label != 0:
                     sed_label_new = np.zeros((pad_width_after_label, 2, 14))
                     doa_label_new = np.zeros((pad_width_after_label, 2, 3))
diff --git a/seld/methods/ein_seld/inference.py b/seld/methods/ein_seld/inference.py
index 46ff392..c7764cd 100644
--- a/seld/methods/ein_seld/inference.py
+++ b/seld/methods/ein_seld/inference.py
@@ -17,6 +17,7 @@ class Inferer(BaseInferer):
         self.af_extractor = af_extractor
         self.model = model
         self.cuda = cuda
+        #self.cuda = False
 
         # Scalar
         scalar_h5_dir = Path(cfg['hdf5_dir']).joinpath(cfg['dataset']).joinpath('scalar')
diff --git a/seld/methods/ein_seld/losses.py b/seld/methods/ein_seld/losses.py
index 95e53a5..dbe6802 100644
--- a/seld/methods/ein_seld/losses.py
+++ b/seld/methods/ein_seld/losses.py
@@ -12,6 +12,7 @@ class Losses:
         self.losses = [BCEWithLogitsLoss(reduction='mean'), MSELoss(reduction='mean')]
         self.losses_pit = [BCEWithLogitsLoss(reduction='PIT'), MSELoss(reduction='PIT')]
 
+
         self.names = ['loss_all'] + [loss.name for loss in self.losses]
     
     def calculate(self, pred, target, epoch_it=0):
diff --git a/seld/methods/ein_seld/metrics.py b/seld/methods/ein_seld/metrics.py
index 69db104..77513f1 100644
--- a/seld/methods/ein_seld/metrics.py
+++ b/seld/methods/ein_seld/metrics.py
@@ -8,6 +8,7 @@ class Metrics(object):
     """Metrics for evaluation
 
     """
+
     def __init__(self, dataset):
 
         self.metrics = []
diff --git a/seld/methods/ein_seld/models/seld.py b/seld/methods/ein_seld/models/seld.py
index 2f84239..80203a0 100644
--- a/seld/methods/ein_seld/models/seld.py
+++ b/seld/methods/ein_seld/models/seld.py
@@ -10,6 +10,7 @@ class EINV2(nn.Module):
         super().__init__()
         self.pe_enable = False  # Ture | False
 
+
         if cfg['data']['audio_feature'] == 'logmel&intensity':
             self.f_bins = cfg['data']['n_mels']
             self.in_channels = 7
diff --git a/seld/methods/ein_seld/training.py b/seld/methods/ein_seld/training.py
index ac43f32..0d17f15 100644
--- a/seld/methods/ein_seld/training.py
+++ b/seld/methods/ein_seld/training.py
@@ -11,6 +11,7 @@ from methods.utils.data_utilities import to_metrics2020_format
 
 class Trainer(BaseTrainer):
 
+
     def __init__(self, args, cfg, dataset, af_extractor, valid_set, model, optimizer, losses, metrics):
 
         super().__init__()
diff --git a/seld/methods/feature.py b/seld/methods/feature.py
index 6795334..a64cee1 100644
--- a/seld/methods/feature.py
+++ b/seld/methods/feature.py
@@ -14,6 +14,7 @@ class LogmelIntensity_Extractor(nn.Module):
             data['sample_rate'], data['n_fft'], data['hop_length'], data['window'], data['n_mels'], \
                 data['fmin'], data['fmax']
         
+
         center = True
         pad_mode = 'reflect'
         ref = 1.0
diff --git a/seld/methods/inference.py b/seld/methods/inference.py
index 0748480..e13fc75 100644
--- a/seld/methods/inference.py
+++ b/seld/methods/inference.py
@@ -14,6 +14,7 @@ class BaseInferer:
         """
         raise NotImplementedError        
 
+
     @staticmethod
     def write_submission(submissions_dir, pred_dict):
         """ Write predicted result to submission csv files
diff --git a/seld/methods/training.py b/seld/methods/training.py
index 63425e6..b7ae5e7 100644
--- a/seld/methods/training.py
+++ b/seld/methods/training.py
@@ -8,6 +8,7 @@ class BaseTrainer:
         """
         raise NotImplementedError
 
+
     def validate_step(self, *args, **kwargs):
         """ Perform a validation step
 
diff --git a/seld/methods/utils/stft.py b/seld/methods/utils/stft.py
index f9ceb78..df6f2a5 100644
--- a/seld/methods/utils/stft.py
+++ b/seld/methods/utils/stft.py
@@ -370,7 +370,7 @@ def spectrogram_STFTInput(input, power=2.0):
     spectrogram = real ** 2 + imag ** 2
 
     if power == 2.0:
-        pass
+        pass #librosa.effects.pitch_shift(spectrogram[1,1,:,0].cpu().numpy() , 24000, n_steps=4)
     else:
         spectrogram = spectrogram ** (power / 2.0)
 
diff --git a/seld/utils/cli_parser.py b/seld/utils/cli_parser.py
index f4892f9..8f60100 100644
--- a/seld/utils/cli_parser.py
+++ b/seld/utils/cli_parser.py
@@ -8,16 +8,13 @@ from termcolor import cprint
 
 def parse_cli_overides():
     """Parse the command-line arguments.
-
     Parse args from CLI and override config dictionary entries
-
     This function implements the command-line interface of the program.
     The interface accepts general command-line arguments as well as
     arguments that are specific to a sub-command. The sub-commands are
     *preprocess*, *train*, *predict*, and *evaluate*. Specifying a
     sub-command is required, as it specifies the task that the program
     should carry out.
-
     Returns:
         args: The parsed arguments.
     """
@@ -26,7 +23,7 @@ def parse_cli_overides():
     # from the config file(s) first and then overidden by the other
     # command-line arguments later.
     parser = argparse.ArgumentParser(
-        description='Event Independent Network for Sound Event Localization and Detection.', 
+        description='Event Independent Network for Sound Event Localization and Detection.',
         add_help=False
     )
     parser.add_argument('-c', '--config_file', help='Specify config file', metavar='FILE')
@@ -40,7 +37,7 @@ def parse_cli_overides():
     subparsers.required = True
     parser_preproc.add_argument('--preproc_mode', choices=['extract_data', 'extract_scalar', 'extract_meta'],
                                 required=True, help='select preprocessing mode')
-    parser_preproc.add_argument('--dataset_type', default='dev', choices=['dev', 'eval'], 
+    parser_preproc.add_argument('--dataset_type', default='dev', choices=['dev', 'eval'],
                                 help='select dataset to preprocess')
     parser_preproc.add_argument('--num_workers', type=int, default=8, metavar='N')
     parser_preproc.add_argument('--no_cuda', action='store_true', help='Do not use cuda.')
@@ -72,4 +69,4 @@ def parse_cli_overides():
 
 def replace_indent(stream):
     stream = "     " + stream
-    return stream.replace("\n", "\n     ")
+    return stream.replace("\n", "\n     ")
\ No newline at end of file
diff --git a/seld/utils/common.py b/seld/utils/common.py
index e4ac213..45eb6ec 100644
--- a/seld/utils/common.py
+++ b/seld/utils/common.py
@@ -87,10 +87,13 @@ def move_model_to_gpu(model, cuda):
 
     """
     # TODO: change DataParallel to DistributedDataParallel
-    model = torch.nn.DataParallel(model)
+    model = torch.nn.DataParallel(model,device_ids=[0])
+    #model = torch.nn.parallel.DistributedDataParallel(model)
     if cuda:
+
         logging.info('Utilize GPUs for computation')
         logging.info('Number of GPU available: {}\n'.format(torch.cuda.device_count()))
+
         model.cuda()
     else:
         logging.info('Utilize CPU for computation')
diff --git a/seld/utils/config.py b/seld/utils/config.py
index 4c12573..2b1d47d 100644
--- a/seld/utils/config.py
+++ b/seld/utils/config.py
@@ -1,6 +1,6 @@
 import logging
 from pathlib import Path
-
+import librosa
 import methods.feature as feature
 import torch.optim as optim
 from methods import ein_seld
@@ -17,7 +17,7 @@ method_dict = {
 
 datasets_dict = {
     'dcase2020task3': Dcase2020task3,
-}
+ }
 
 
 def store_config(output_path, config):
@@ -63,10 +63,13 @@ def get_generator(args, cfg, dataset, generator_type):
     if generator_type == 'train':
 
         subset = method_dict[cfg['method']].data.UserDataset(args, cfg, dataset, dataset_type='train')
-        if 'pitchshift' in cfg['data_augmentation']['type']:
-            augset = method_dict[cfg['method']].data.UserDataset(args, cfg, dataset, dataset_type='train_pitchshift')
+        #if 'pitchshift' in cfg['data_augmentation']['type']:
+        #    augset = method_dict[cfg['method']].data.UserDataset(args, cfg, dataset, dataset_type='train_pitchshift')
+        #    subset = ConcatDataset([subset, augset])
+        if 'train_rotate_channel' in cfg['data_augmentation']['type']:
+            augset = method_dict[cfg['method']].data.UserDataset(args, cfg, dataset, dataset_type='train_rotate_channel')
             subset = ConcatDataset([subset, augset])
-        
+
         batch_sampler = method_dict[cfg['method']].data.UserBatchSampler(
             clip_num=len(subset), 
             batch_size=cfg['training']['batch_size'], 
@@ -97,7 +100,7 @@ def get_generator(args, cfg, dataset, generator_type):
             dataset=subset,
             batch_size=cfg['inference']['batch_size'],
             shuffle=False,
-            num_workers=args.num_workers,
+            num_workers=8, #args.num_workers
             collate_fn=method_dict[cfg['method']].data.collate_fn_test,
             pin_memory=True
         )
diff --git a/seld/utils/datasets.py b/seld/utils/datasets.py
index 467813d..99098b5 100644
--- a/seld/utils/datasets.py
+++ b/seld/utils/datasets.py
@@ -14,6 +14,7 @@ class Dcase2020task3:
             'mic': self.root_dir.joinpath('mic_dev'),
             'meta': self.root_dir.joinpath('metadata_dev'),            
         }
+
         self.dataset_dir['eval'] = {
             'foa': self.root_dir.joinpath('foa_eval'),
             'mic': self.root_dir.joinpath('mic_eval'),
